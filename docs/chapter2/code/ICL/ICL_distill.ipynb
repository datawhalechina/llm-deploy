{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代码参考了 https://github.com/facebookresearch/MetaICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy\n",
    "# %pip install pickle\n",
    "# %pip install transformers==4.28.1\n",
    "# %pip install torch==2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import Adafactor, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/PJLAB/gaoyufei/workdir/llm-deploy/docs/chapter2/code/ICL\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载数据并解压到同级目录\n",
    "# https://github.com/gyfffffff/llm-deploy/releases/download/icl_data/icl_data.zip\n",
    "\n",
    "# 使用Linux 系统的同学也可以直接运行：\n",
    "# !wget https://github.com/gyfffffff/llm-deploy/releases/download/icl_data/icl_data.zip\n",
    "\n",
    "# !unzip icl_data.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'task': 'dream',\n",
       "   'input': \"Based on the conversation, what is the most probable relationship between the speakers? [SEP] Heather: Ron, what are you doing? Ron: Ah, nothing. I'm just looking up some information on the Internet. Heather: Like what? Let me see. Ron: No, no, it's okay. I mean, you know ... Heather: Baldness? What are you looking that up for? [Well, you know ... ] I ... I mean, you're not that bad off. Ron: Ah, there you go. Bringing it up again! Heather: No. I mean it. You look great! Honestly, it's not that bad. Ron: Hey, I get enough of it from friends, and the people at work, and now from you! Heather: Well, maybe you could wear a toupee? I think you'd look great. Ron: Oh no. And have it slip off my head on to my date's dinner plate as I lean over to kiss her? Uh-uh. Heather: Well, have you ever thought about seeking medical advice? There are new advances in medicines that not only retard hair loss, but help regenerate new growth. Ron: Ah, I still don't give much credibility to medical treatment to prevent permanent hair loss. Heather: Well, what about accepting the fact that you're just losing your hair? Ron: I just can't give up hope. I know appearances shouldn't matter, but I don't know. I just feel that women just avoid me. Heather: Come on. You can't be serious. Ron: No really. I've seen it many times. It just, I don't know ... [SEP]\",\n",
       "   'output': 'brother and sister',\n",
       "   'options': ['old friends', 'brother and sister', 'colleagues from work']},\n",
       "  {'task': 'dream',\n",
       "   'input': \"Where are the two speakers? [SEP] M: Listen! The show is starting. W: How do you know? M: Can't you hear the band? They are playing their instruments. W: Sure, yes. Let's hurry up. [SEP]\",\n",
       "   'output': 'Outside a theatre.',\n",
       "   'options': ['Outside a theatre.', 'In a bank.', 'At a show.']},\n",
       "  {'task': 'dream',\n",
       "   'input': \"What is one thing the extended warranty would NOT cover on the vehicle according to the conversation? [SEP] Car Salesman: Oh. Hi there. A beauty, isn't she? Woman: Well ... Car Salesman: Do you want to take her a test ride? Woman: Well ... Um. How old is it? Car Salesman: Well, it's only three years old? Woman: And what's the mileage? Car Salesman: Uh, let me check. Oh yes. 75,000 miles. Woman: 75,000 miles? That's quite a bit for a car that's only three years old. Car Salesman: Well, once you're in the driver's seat, you'll fall in love with her. Get in. Woman: Ugh ... Uh, I can't seem to get the door open. [Ah, it's okay.] It could be broken. Car Salesman: Ah, just give her a little tap. Ugh. Now she's opened. Woman: Great. A door I have to beat up to open. Car Salesman: Hey. Get in and start her up. [Woman tries to start the car ...] Car Salesman: [Um] Well, it's probably the battery. I know she has enough gas in her, and I had our mechanic check her out just yesterday. Try it again. Woman: Uh. It sounds a little rough to me. [Well ...] How much is this minivan anyway? Car Salesman: Oh. It's a real bargain today and tomorrow only at $15,775, plus you get the extended warranty covering defects, wear, and tear beyond the normal maintenance on the vehicle for an extra $500 for the next 30,000 miles. [Oh ...] with a few minor exclusions. Woman: Like ...? Car Salesman: Well, I mean, it covers everything except for the battery, and light bulbs, and brake drums, exhaust system, trim and moldings, upholstery and carpet, paint, tires ... Well, a short list, you know. Woman: Uh. Well, almost $16,000 is a little out of my price range, plus the seats covers are torn a little. Car Salesman: Well, hey, I might be able to talk the manager into lowering the price another two hundred dollars, but that's about all. Woman: No thanks. I think I'll just keep looking. [SEP]\",\n",
       "   'output': 'a worn out brake drum',\n",
       "   'options': ['a faulty oil pump',\n",
       "    'a malfunctioning gage',\n",
       "    'a worn out brake drum']},\n",
       "  {'task': 'dream',\n",
       "   'input': \"Which is the right order of the places the speakers have been to? [SEP] W: I can't find my purse. M: Oh, no. Maybe you left it at the bank. You took it out when you cashed the cheque. W: But I remember having it after that. M: Well, you paid for the coffee when we were in that coffee shop. Remember? W: That's right. I remember feeling it to my pocket as we walked out. M: And then, we dropped in ... bookshop... W: Yes. But I remember taking a piece of paper out of my purse to write down the name of the book on it. I think I must have left the purse on the shelf. I'll call the bookshop and see if they found it. [SEP]\",\n",
       "   'output': 'Bank---coffee shop---bookshop',\n",
       "   'options': ['Coffee shop---bookshop---bank',\n",
       "    'Bookshop---bank---coffee shop',\n",
       "    'Bank---coffee shop---bookshop']},\n",
       "  {'task': 'dream',\n",
       "   'input': 'What does the police officer suggest at the end of the story? [SEP] Police Officer: Hello. 24th Precinct. Officer Jones speaking. Man: Help. Yeah, uh, it was wild, I mean really bizarre. Police Officer: Calm down sir! Now, what do you want to report? Man: Well, I\\'d like to report a UFO sighting. Police Officer: A what? Man: What do you mean \"what?\" An unidentified flying object! Police Officer: Wait, tell me exactly what you saw. Man: Well, I was driving home from a party about three hours ago, so it was about 2:00 AM, when I saw this bright light overhead. Police Officer: Okay. And then what happened? Man: Oh, man. Well, it was out of this world. I stopped to watch the light when it disappeared behind a hill about a kilometer ahead of me. Police Officer: Alright. Then what? Man: Well, I got back in my car and I started driving toward where the UFO landed. Police Officer: Now, how do you know it was a UFO? Perhaps you only saw the lights of an airplane [No], or the headlights of an approaching car [No]. Things like that happen, you know. Man: Well if it was that, how do you explain \"the BEAST\"? Police Officer: What do you mean, \"the BEAST\"? Man: Okay. I kept driving for about five minutes when all of a sudden, this giant, hairy creature jumped out in front of my car. Police Officer: Oh, yeah. Then what? Man: Well, then, the beast picked up the front of my car and said, \"Get out of the car. I\\'m taking you to my master!\" Something like that. Police Officer: Wow? A hairy alien who can speak English! Come on! Man: I\\'m not making this up, if that\\'s what you\\'re suggesting. Then, when I didn\\'t get out of the car, the beast opened the car door, carried me on his shoulders to this round-shaped flying saucer, and well, that\\'s when I woke up along side the road. The beast must have knocked me out and left me there. Police Officer: Well, that\\'s the best story I\\'ve heard all night, sir. Now, have you been taking any medication, drugs, or alcohol in the last 24 hours? You mentioned you went to a party. Man: What? Well, I did have a few beers, but I\\'m telling the truth. Police Officer: Okay, okay. We have a great therapist that deals with THESE kinds of cases. Man: I\\'m not crazy. Police Officer: Well, we\\'ll look into your story. Thank you. [SEP]',\n",
       "   'output': 'The man should seek counseling.',\n",
       "   'options': ['They should call the fire department.',\n",
       "    'The man should seek counseling.',\n",
       "    'The man should contact the newspaper.']},\n",
       "  {'task': 'dream',\n",
       "   'input': \"What does the woman mean? [SEP] M: I am surprised to see you using the record player I was going to throw away! W: It still works. You'd better get fid of these wasteful habits. [SEP]\",\n",
       "   'output': 'The man should abandon his bad habit.',\n",
       "   'options': ['The record player is still useful.',\n",
       "    \"The man shouldn't have thrown it away.\",\n",
       "    'The man should abandon his bad habit.']},\n",
       "  {'task': 'dream',\n",
       "   'input': 'What time did the man get up this morning? [SEP] F: When do you usually get up, Peter? M: I usually get up at half past six, but this morning I got up a quarter later. [SEP]',\n",
       "   'output': 'At 6:45.',\n",
       "   'options': ['At 6:30.', 'At 6:15.', 'At 6:45.']},\n",
       "  {'task': 'dream',\n",
       "   'input': \"What is the woman's major? [SEP] W: What's your major? M: Hotel management. W: What do you want to do when you graduate? M: I'd like to work for a hotel or a travel agency in this area. How about you? W: At first I wanted to major in French or history, but I realized I might have a hard time finding a job, so I major in computer science. With the right skills, getting a job shouldn't be so difficult. M: Do you have a part-time job to support yourself through school? W: Fortunately for me, I received a four-year academic scholarship. M: Wow. That's great. W: Are you working your way through school? M: Yeah. I work as a cook in a restaurant near campus three times a week. [SEP]\",\n",
       "   'output': 'Computer science.',\n",
       "   'options': ['History.', 'French.', 'Computer science.']},\n",
       "  {'task': 'dream',\n",
       "   'input': \"What may be the relationship between the two speakers? [SEP] M: I'm going to the store. What do we need? W: Um, what do you want for dinner tonight? M: How about chicken? W: We had chicken last night. M: Yeah, that was really good. I want some more of that. W: I'll make more next week. How about beef? M: I've got sick of that. W: What about noodles for tonight? M: Sure, I'd like that. I'll get some noodles and some mushrooms and onions. W: Sounds good. Oh, and get some potatoes and tomatoes for a salad. M: Do we have dessert? W: No, we don't. Why don't you get a melon? [SEP]\",\n",
       "   'output': 'Wife and husband.',\n",
       "   'options': ['Wife and husband.',\n",
       "    'Boss and secretary.',\n",
       "    'Customer and shop assistant.']},\n",
       "  {'task': 'dream',\n",
       "   'input': \"What can we know about the singing group? [SEP] M: What do you usually do in your spare time? W: I have joined a singing group and we practice two afternoons each week. M: That's a very interesting thing to do. W: Yes. It's a small group of only fifteen girls and boys. But nobody has been late for the practice. M: I see. How many songs can you sing now? W: We started only three months ago, but we've practiced eight songs. M: Are you going to perform any time? W: Yes, there will be a school show next month and we are going to sing a song in it. We are practicing a new song now. It's written by one of the singers in our group. M: Really? What's the name of the song? W: Sunny Days. M: That's a nice name. I hope to listen to it soon. Can I visit you when you practice? [SEP]\",\n",
       "   'output': 'Every member comes on time for each practice.',\n",
       "   'options': [\"It's made up of 15 girls.\",\n",
       "    'It practices once every other week.',\n",
       "    'Every member comes on time for each practice.']},\n",
       "  {'task': 'dream',\n",
       "   'input': \"What kind of transport does her husband prefer to take? [SEP] Susan: Are you looking forward to your trip to Canada, Julie? I hear you have planned it for a long time. Julie: I can't wait to see Canada, Susan, but I'm somehow scared of the journey. My husband insists on flying but I want to sail. Planes make me nervous. Susan: There's nothing to be frightened. How many planes fly across the Atlantic every day? Julie: I've no idea. Hundreds, I suppose. But any accident in a flight will lead to tragedy. Susan: And how often do you hear of a crash? Once or twice a year? Julie: Yes, but planes fly so high and fast that once was enough. Susan: Look, there are more road casualties per day than air deaths per year. Air transport is really safe compared with road transport. Julie: I'd still prefer to go by sea. Ships may not travel fast and at least you can relax. I'd love a trip on a luxury liner like the Queen Elizabeth II. Susan: It's fine if you're a good sailor. But have you ever traveled far in a rough sea? Julie: No. I've only been in a boat once. I sailed down the River Thames on a sightseeing tour. But in many eases I'd rather to be sea - sick than dead. [SEP]\",\n",
       "   'output': 'By air.',\n",
       "   'options': ['By air.', 'By bus.', 'By train.']},\n",
       "  {'task': 'dream',\n",
       "   'input': \"How did the man mention in the newspaper try to win further trust from the victims? [SEP] W: Gosh! Have you seen this, Richard? M: See what? W: In the paper. It says, there is a man going around pretending he's from the electricity board. He's been calling at people's homes, saying he is coming to check that all their appliances are safe. Then he gets around them to make him a cup of tea, and while they are out of the room he steals their money, handbag whatever and makes off with it. M: But you know, Jane, it's partly their own fault. You should never let anyone like that in unless you're expecting them. W: It's all very well to say that. But someone comes to the door, and says electricity or gas and you automatically think they are OK, especially if they flash a card to you M: Does this man have an ID then? W: Yes, that's just it. It seems he used to work for the electricity board at one time. According to the paper the police are warning people especially pensioners not to admit anyone unless they have an appointment. It's a bit sad. One old lady told them she'd just been to the post–office to draw her pension when he called. She said he must have followed her home. He stole the whole lot. M: But what does he look like? Surely they must have a description. W: Oh, yes they have. Let's see, in his thirties, tall, bushy dark hair, slight northern accent, sounds a bit like you actually. [SEP]\",\n",
       "   'output': 'Showing them his ID.',\n",
       "   'options': ['Speaking with a proper accent.',\n",
       "    'Showing them his ID.',\n",
       "    'Making friends with them.']},\n",
       "  {'task': 'dream',\n",
       "   'input': \"What probable caused the man's stomachache? [SEP] W: You don't feel very well, do you? You look pale. Have you got a cold? M: Oh, no, but my stomachaches. Maybe the seafood doesn't agree with me. [SEP]\",\n",
       "   'output': 'The sea food.',\n",
       "   'options': ['The pear.', 'The weather.', 'The sea food.']},\n",
       "  {'task': 'dream',\n",
       "   'input': 'Why was Carl at the hospital? [SEP] W: I saw Carl at the hospital. I wonder if his wife is ill. M: No, she is fine. His daughter has just had a baby and he was visiting her, I think. [SEP]',\n",
       "   'output': 'He was visiting his daughter.',\n",
       "   'options': ['He was meeting a doctor.',\n",
       "    'He was looking for his wife.',\n",
       "    'He was visiting his daughter.']},\n",
       "  {'task': 'dream',\n",
       "   'input': \"What was Mary doing? [SEP] W: Did you see Mary somewhere around? M: Yes, she is in the campus bank, applying for the student's loan. [SEP]\",\n",
       "   'output': 'Applying for financial aid.',\n",
       "   'options': ['Reading on the campus lawn.',\n",
       "    'Depositing money in the bank.',\n",
       "    'Applying for financial aid.']},\n",
       "  {'task': 'dream',\n",
       "   'input': 'Why was the man relieved at last? [SEP] M: Something very unusual happened to me this morning. W: Really? What was it? M: I was studying in the classroom when Tom came rushing in. W: Yes? M: He told me there was a telegram for me at the gate house! Someone was ill in my family. W: Goodness me! I hate telegrams. They seldom bring any good news. M: Yes, that\\'s just how I felt. My legs turned to water, so I asked Tom to accompany me to the school gate. W: I can imagine how you felt. M: On my way to the school gate I was thinking terrible things. What could have happened at home, you know, and all that. W: Yes, of course. I understand. M: So when I tore the telegram open, my fingers were trembling. W: Dear me! M: But when I read the telegram, I just couldn\\'t make heads or tails out of it. W: How strange! What did it say? M: It said: Return immediately. Uncle seriously ill. W: Oh, I am sorry to hear that. M: But the surprising part about it is that I have no uncle. W: Indeed! M: I could hardly believe my own eyes. But it was written there in black and white. Then I happened to glance at the address to a \"Carl\", not \"Carol\". W: Well, I never! M: You can never imagine how relieved I was. W: Yes. What a relief! But what did Tom have to say? M: Tom was so embarrassed. He kept apologizing all the way back. W: He has always been quite careless. [SEP]',\n",
       "   'output': 'Because this telegram was not addressed to him.',\n",
       "   'options': ['Because the telegram was a false one.',\n",
       "    'Because this telegram was not addressed to him.',\n",
       "    \"Because his uncle wasn't ill at all.\"]}],\n",
       " [{'task': 'wiki_qa',\n",
       "   'input': 'question: what were the disease like in the great depression ? [SEP] answer: Some economies started to recover by the mid-1930s.',\n",
       "   'output': 'false',\n",
       "   'options': ['false', 'true']},\n",
       "  {'task': 'wiki_qa',\n",
       "   'input': \"question: what is darwin's origin of species [SEP] answer: The book was written for non-specialist readers and attracted widespread interest upon its publication.\",\n",
       "   'output': 'false',\n",
       "   'options': ['false', 'true']},\n",
       "  {'task': 'wiki_qa',\n",
       "   'input': 'question: who sings backup on no one to blame howard jones [SEP] answer: Howard Jones (born John Howard Jones, 23 February 1955, Southampton , Hampshire , England) is a British musician, singer and songwriter.',\n",
       "   'output': 'false',\n",
       "   'options': ['false', 'true']},\n",
       "  {'task': 'wiki_qa',\n",
       "   'input': 'question: What are the two initials of the first US president to resign from office [SEP] answer: The president is frequently described as the most powerful person in the world.',\n",
       "   'output': 'false',\n",
       "   'options': ['false', 'true']},\n",
       "  {'task': 'wiki_qa',\n",
       "   'input': 'question: how many feet in an acre [SEP] answer: In the United States both the international acre and the slightly different US survey acre are in use.',\n",
       "   'output': 'false',\n",
       "   'options': ['false', 'true']},\n",
       "  {'task': 'wiki_qa',\n",
       "   'input': 'question: who pulmonary hypertension [SEP] answer: In medicine , pulmonary hypertension (PH) is an increase of blood pressure in the pulmonary artery , pulmonary vein , or pulmonary capillaries, together known as the lung vasculature , leading to shortness of breath , dizziness , fainting , and other symptoms, all of which are exacerbated by exertion.',\n",
       "   'output': 'false',\n",
       "   'options': ['false', 'true']},\n",
       "  {'task': 'wiki_qa',\n",
       "   'input': 'question: what happened to stevie ray vaughan [SEP] answer: His guitar playing reflected the classic blues and pentatonic scales.',\n",
       "   'output': 'false',\n",
       "   'options': ['false', 'true']},\n",
       "  {'task': 'wiki_qa',\n",
       "   'input': 'question: how is paper measured [SEP] answer: Expressed in grams per square meter (g/m2), paper density is also known as grammage.',\n",
       "   'output': 'false',\n",
       "   'options': ['false', 'true']},\n",
       "  {'task': 'wiki_qa',\n",
       "   'input': 'question: what is the highest point in oahu [SEP] answer: Fly-around tour of the island',\n",
       "   'output': 'false',\n",
       "   'options': ['false', 'true']},\n",
       "  {'task': 'wiki_qa',\n",
       "   'input': 'question: how did j.p morgan become a wealthy man? [SEP] answer: He was the leading financier of the Progressive Era , and his dedication to efficiency and modernization helped transform American business.',\n",
       "   'output': 'false',\n",
       "   'options': ['false', 'true']},\n",
       "  {'task': 'wiki_qa',\n",
       "   'input': 'question: where was princess diana from [SEP] answer: While married she bore the titles Princess of Wales , Duchess of Cornwall , Duchess of Rothesay , Countess of Chester and Baroness of Renfrew .',\n",
       "   'output': 'false',\n",
       "   'options': ['false', 'true']},\n",
       "  {'task': 'wiki_qa',\n",
       "   'input': \"question: when did secretariat win [SEP] answer: Secretariat's grandsire, Nasrullah , is also the great-great-grandsire of 1977 Triple Crown winner Seattle Slew .\",\n",
       "   'output': 'false',\n",
       "   'options': ['false', 'true']},\n",
       "  {'task': 'wiki_qa',\n",
       "   'input': 'question: what did benedict arnold die [SEP] answer: Benedict Arnold (June 14, 1801) was a general during the American Revolutionary War who originally fought for the American Continental Army but defected to the British Army .',\n",
       "   'output': 'false',\n",
       "   'options': ['false', 'true']},\n",
       "  {'task': 'wiki_qa',\n",
       "   'input': \"question: What Are Mnemonic Devices [SEP] answer: Ancient Greeks and Romans distinguished between two types of memory: the 'natural' memory and the 'artificial' memory.\",\n",
       "   'output': 'false',\n",
       "   'options': ['false', 'true']},\n",
       "  {'task': 'wiki_qa',\n",
       "   'input': 'question: who starred in my fair lady [SEP] answer: The story concerns Eliza Doolittle, a Cockney flower girl who takes speech lessons from professor Henry Higgins, a phoneticist , so that she may pass as a well-born lady.',\n",
       "   'output': 'false',\n",
       "   'options': ['false', 'true']},\n",
       "  {'task': 'wiki_qa',\n",
       "   'input': 'question: what is the disease osteonecrosis of the jaw? [SEP] answer: Osteonecrosis of the jaws associated with bisphosphonate therapy, required by some cancer treatment regimens, has been identified and defined as a pathological entity since 2003.',\n",
       "   'output': 'true',\n",
       "   'options': ['false', 'true']}]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取数据\n",
    "train_data_files = [\"data/icl_data/dream/dream_16_13_dev.jsonl\", \"data/icl_data/wiki_qa/wiki_qa_16_13_dev.jsonl\"]\n",
    "train_data = []\n",
    "for train_data_file in train_data_files:\n",
    "    with open(train_data_file, \"r\") as f:\n",
    "        data = []\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    train_data.append(data)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载模型 (这里教师模型和学生模型都用GPT-2)\n",
    "\n",
    "# windows\n",
    "# %pip install -U \"huggingface-hub[cli]\"\n",
    "# !$env:HF_ENDPOINT = \"https://hf-mirror.com\"\n",
    "# !huggingface-cli download --resume-download openai-community/gpt2 --local-dir ../../models/GPT-2\n",
    "\n",
    "# linux\n",
    "# %pip install -U \"huggingface-hub[cli]\"\n",
    "# !export HF_ENDPOINT=https://hf-mirror.com\n",
    "# !huggingface-cli download --resume-download openai-community/gpt2 --local-dir ../../models/GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=1024\n",
    "max_length_per_example=256\n",
    "model_path = \"../../models/GPT-2\"\n",
    "batch_size = 4\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 定义数据处理函数\n",
    "def prepro_sentence_pair_single(ids1, ids2, max_length,\n",
    "                                bos_token_id, eos_token_id,\n",
    "                                allow_truncation=False):\n",
    "\n",
    "    #if bos_token_id is not None:\n",
    "    #    ids1 = [bos_token_id] + ids1\n",
    "    #if eos_token_id is not None:\n",
    "    #    ids2 = ids2 + [eos_token_id]\n",
    "    if allow_truncation and len(ids1)+len(ids2) > max_length:\n",
    "        ids1 = ids1[len(ids1)+len(ids2)-max_length:] # len = max_length-len(ids2)\n",
    "        assert len(ids1)+len(ids2)==max_length\n",
    "\n",
    "    n_mask = max_length-len(ids1)-len(ids2)\n",
    "    assert n_mask>=0, (max_length, len(ids1), len(ids2))\n",
    "    input_ids = ids1+ids2+[0 for _ in range(n_mask)]\n",
    "    attention_mask = [1 for _ in ids1+ids2] + [0 for _ in range(n_mask)]\n",
    "    token_type_ids = [0 for _ in ids1] + [1 for _ in ids2] + [0 for _ in range(n_mask)]\n",
    "    return input_ids, attention_mask, token_type_ids\n",
    "\n",
    "\n",
    "def _prepro_each_datapoint(dp, is_first=True, is_training=False, for_demonstrations=False):\n",
    "    dp = dp.copy()\n",
    "\n",
    "    no_label = np.all([option==\"\" for option in dp[\"options\"]])\n",
    "    no_input = dp[\"input\"]==\"\"\n",
    "    if not is_first:\n",
    "        dp[\"output\"] = \"\\n\\n\\n\" + dp[\"output\"]\n",
    "        if \"options\" in dp:\n",
    "            dp[\"options\"] = [\"\\n\\n\\n\" + opt for opt in dp[\"options\"]]\n",
    "    if not no_input:\n",
    "        if not no_label:\n",
    "            dp[\"input\"] = \"\\n\" + dp[\"input\"]\n",
    "\n",
    "    input_tokens = tokenizer(dp[\"input\"])[\"input_ids\"]\n",
    "\n",
    "    if is_training or for_demonstrations:\n",
    "        output_tokens = tokenizer(dp[\"output\"])[\"input_ids\"]\n",
    "\n",
    "        if \"task\" in dp:\n",
    "            if len(input_tokens)>=max_length_per_example - 2 - len(output_tokens):\n",
    "                if dp[\"task\"].startswith(\"inst:\") and len(input_tokens)<len(output_tokens):\n",
    "                    output_tokens = output_tokens[:max_length_per_example - 2 - len(input_tokens)]\n",
    "                else:\n",
    "                    input_tokens = input_tokens[:max_length_per_example - 2 - len(output_tokens)]\n",
    "\n",
    "        assert len(input_tokens)+len(output_tokens)+2<=max_length_per_example, \\\n",
    "            (dp.get(\"task\", None), len(input_tokens), len(output_tokens), max_length_per_example)\n",
    "\n",
    "        return output_tokens, input_tokens\n",
    "\n",
    "\n",
    "    else:\n",
    "        assert len(dp[\"options\"])>=2, dp\n",
    "        assert dp[\"output\"] in dp[\"options\"]\n",
    "        option_tokens = [tokenizer(option)[\"input_ids\"] for option in dp[\"options\"]]\n",
    "        option_length = np.max([len(option) for option in option_tokens])\n",
    "\n",
    "        if len(input_tokens)>=max_length_per_example - 2 - option_length:\n",
    "            input_tokens = input_tokens[:max_length_per_example - 2 - option_length]\n",
    "\n",
    "        input_tokens = [input_tokens for _ in option_tokens]\n",
    "        output_tokens = option_tokens\n",
    "        option_tokens = [dp[\"options\"].index(dp[\"output\"])]\n",
    "\n",
    "        return output_tokens, input_tokens, option_tokens\n",
    "\n",
    "\n",
    "def _tensorize_for_training(train_data):\n",
    "    for dp in train_data:  # train_data： [{\"input\": str, \"output\": str}, ...]\n",
    "        assert type(dp)==dict, (\"Each example should be a dictionary\", dp)\n",
    "        assert \"input\" in dp and \"output\" in dp, (\"Training example should contain input and output\", dp)\n",
    "\n",
    "    # each datapoint: passage, question, options, output\n",
    "    bos_token_id = tokenizer.bos_token_id\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    input_ids, attention_mask, token_type_ids = [], [], []\n",
    "\n",
    "    for dp in train_data:\n",
    "        inputs, outputs = _prepro_each_datapoint(\n",
    "            dp, is_first=True, is_training=True)\n",
    "\n",
    "        encoded = prepro_sentence_pair_single(\n",
    "            inputs, outputs, max_length, bos_token_id, eos_token_id)\n",
    "\n",
    "        input_ids.append(encoded[0])\n",
    "        attention_mask.append(encoded[1])\n",
    "        token_type_ids.append(encoded[2])\n",
    "\n",
    "    return dict(input_ids=torch.LongTensor(input_ids),\n",
    "                attention_mask=torch.LongTensor(attention_mask),\n",
    "                token_type_ids=torch.LongTensor(token_type_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish saving preprocessed data ...\n"
     ]
    }
   ],
   "source": [
    "# 数据转为tensor\n",
    "\n",
    "# 数据格式：\n",
    "# [[{}, {}, ...], [{}, {}, ...], ...]  \n",
    "# 每个子列表是一个数据集，每个字典是一个数据点\n",
    "\n",
    "\n",
    "def tensorize_for_training(train_data, is_training=True):\n",
    "    inputs = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "    \n",
    "    # 张量化每一条数据\n",
    "    for in_ in train_data:\n",
    "        out = _tensorize_for_training(in_)\n",
    "        for key in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]:\n",
    "            inputs[key] += out[key].numpy().tolist()\n",
    "\n",
    "    # 数据打乱\n",
    "    N = len(inputs[\"input_ids\"])\n",
    "    indices = np.random.permutation(range(N))\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = np.array(v)[indices]\n",
    "\n",
    "    # 保存数据\n",
    "    with open(\"data/preprocessed_data.kpl\", \"wb\") as f:\n",
    "        pkl.dump({k:v for k, v in inputs.items()}, f)\n",
    "    print(\"Finish saving preprocessed data ...\")\n",
    "\n",
    "    # 定义dataset，dataloader\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.LongTensor(v)\n",
    "    shape = inputs[\"input_ids\"].shape\n",
    "    for v in inputs.values():\n",
    "        assert v.shape==shape\n",
    "    if \"labels\" in inputs:\n",
    "        dataset = TensorDataset(inputs[\"input_ids\"], inputs[\"attention_mask\"], inputs[\"token_type_ids\"], inputs[\"labels\"])\n",
    "    else:\n",
    "        dataset = TensorDataset(inputs[\"input_ids\"], inputs[\"attention_mask\"], inputs[\"token_type_ids\"])\n",
    "    if is_training:\n",
    "        sampler=RandomSampler(dataset)\n",
    "    else:\n",
    "        sampler=SequentialSampler(dataset)\n",
    "    dataloader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
    "    return dataloader\n",
    "dataloader = tensorize_for_training(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载wikitext数据集\n",
    "from datasets import load_dataset\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "dataset = load_dataset(\"./data/wikitext\", 'wikitext-103-raw-v1', split='validation')\n",
    "\n",
    "def encode(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "\n",
    "dataset = dataset.map(encode, batched=True)\n",
    "\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"text\"])\n",
    "text_dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['', ' = Homarus gammarus = \\n', '', ' Homarus gammarus , known as the European lobster or common lobster , is a species of clawed lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws . In life , the lobsters are blue , only becoming \" lobster red \" on cooking . Mating occurs in the summer , producing eggs which are carried by the females for up to a year before hatching into planktonic larvae . Homarus gammarus is a highly esteemed food , and is widely caught using lobster pots , mostly around the British Isles . \\n'], 'input_ids': tensor([[50256, 50256, 50256,  ..., 50256, 50256, 50256],\n",
      "        [  796,  8074, 20272,  ..., 50256, 50256, 50256],\n",
      "        [50256, 50256, 50256,  ..., 50256, 50256, 50256],\n",
      "        [ 8074, 20272,  9106,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for batch in text_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/20/2024 14:06:31 - INFO - __main__ - output\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# 指定日志路径\n",
    "\n",
    "out_dir = \"output\"\n",
    "log_file = f\"{out_dir}/log.txt\"\n",
    "handlers = [logging.StreamHandler()]\n",
    "handlers.append(logging.FileHandler(log_file))\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=handlers)\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(out_dir)\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# 定义不同的损失函数\n",
    "device = torch.device(\"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "def run_model_meta_icl(input_ids, attention_mask, token_type_ids, labels=None):\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits[..., :-1, :].contiguous()\n",
    "\n",
    "        if labels is None:\n",
    "            labels = input_ids\n",
    "        labels = labels[..., 1:].contiguous()\n",
    "        label_mask = token_type_ids[..., 1:].contiguous()\n",
    "\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        losses = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1)) # [batch_size, length]\n",
    "\n",
    "        losses = losses.view(logits.size(0), logits.size(1)) * label_mask\n",
    "        return torch.sum(losses, axis=1) / torch.sum(label_mask, axis=1)\n",
    "\n",
    "def run_model_icl_distill(input_ids, attention_mask, token_type_ids, text_input_ids, text_attention_mask, step, labels=None):\n",
    "        beta = 0.2\n",
    "        stu_outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        stu_logits = stu_outputs.logits[..., :-1, :].contiguous()\n",
    "        prob_stu = torch.nn.functional.softmax(stu_logits, dim=-1)\n",
    "\n",
    "        tea_outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        tea_logits = tea_outputs.logits[..., :-1, :].contiguous()\n",
    "        log_tea_prob = torch.nn.functional.log_softmax(tea_logits, dim=-1)\n",
    "\n",
    "        # soft icl loss: 学生和教师输出的交叉熵\n",
    "        soft_icl_loss = -torch.sum(prob_stu * log_tea_prob, axis=-1)\n",
    "        # soft_loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        # print(stu_logits.view(-1, stu_logits.size(-1)).shape, soft_targets.view(-1).shape)\n",
    "        # soft_icl_loss = soft_loss_fct(stu_logits.view(-1, stu_logits.size(-1)), soft_targets.view(-1))\n",
    "        \n",
    "        # soft_losses = soft_loss_fct(stu_logits.view(-1, stu_logits.size(-1)), soft_targets.view(-1).argmax(-1))\n",
    "\n",
    "        # soft_losses = soft_losses.view(stu_logits.size(0), stu_logits.size(1))\n",
    "        # soft_losses = torch.sum(soft_losses, axis=1)\n",
    "\n",
    "        # soft lm loss: 学生和教师输出的交叉熵\n",
    "        stu_text_outputs = model(input_ids=text_input_ids, attention_mask=text_attention_mask)\n",
    "        stu_text_logits = stu_text_outputs.logits[..., :-1, :].contiguous()\n",
    "        prob_stu_text = torch.nn.functional.softmax(stu_text_logits, dim=-1)\n",
    "        \n",
    "        tea_text_outputs = model(input_ids=text_input_ids, attention_mask=text_attention_mask)\n",
    "        tea_text_logits = tea_text_outputs.logits[..., :-1, :].contiguous()\n",
    "        log_tea_prob_text = torch.nn.functional.softmax(tea_text_logits, dim=-1)\n",
    "\n",
    "        soft_lm_loss = -torch.sum(prob_stu_text * log_tea_prob_text, axis=-1)\n",
    "    \n",
    "        # soft_text_loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        # soft_text_losses = soft_text_loss_fct(stu_text_logits.view(-1, stu_text_logits.size(-1)), soft_text_targets.view(-1))\n",
    "\n",
    "        # soft_text_losses = soft_text_losses.view(stu_text_logits.size(0), stu_text_logits.size(1))\n",
    "\n",
    "        soft_loss = soft_icl_loss + beta * soft_lm_loss\n",
    "\n",
    "        # hard icl loss: 学生和ground-truth的交叉熵\n",
    "        labels = labels[..., 1:].contiguous()\n",
    "        label_mask = token_type_ids[..., 1:].contiguous()\n",
    "\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        losses = loss_fct(stu_logits.view(-1, stu_logits.size(-1)), labels.view(-1)) # [batch_size, length]\n",
    "\n",
    "        hard_icl_losses = losses.view(stu_logits.size(0), stu_logits.size(1)) * label_mask\n",
    "        hard_icl_loss = torch.sum(hard_icl_losses, axis=1) / torch.sum(label_mask, axis=1)\n",
    "\n",
    "        # hard lm loss: 学生和ground-truth的交叉熵\n",
    "        text_outputs = model(input_ids=text_input_ids, attention_mask=text_attention_mask)\n",
    "        hard_lm_loss = text_outputs.loss\n",
    "        \n",
    "        hard_loss = hard_icl_loss + beta * hard_lm_loss\n",
    "\n",
    "        # \"We linearly decrease the weight of hard-label loss α(t) and linearly increase the weight of soft-label loss during training.\"\n",
    "        alpha = 1-step/len(dataloader)\n",
    "\n",
    "        loss = alpha * torch.sum(hard_loss) + (1-alpha) * soft_loss\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/PJLAB/gaoyufei/anaconda3/envs/babyllama/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "save_period = 10\n",
    "log_period = 5\n",
    "batch_size = 4\n",
    "num_training_steps = 1000\n",
    "epoches = 100\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-05, eps=1e-08)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)   \n",
    "\n",
    "model.train()\n",
    "\n",
    "def save(step):\n",
    "        model_state_dict = {key[7:] if key.startswith(\"module.\") else key: value.cpu()\n",
    "                        for key, value in model.state_dict().items()}\n",
    "        torch.save(model_state_dict, os.path.join(out_dir, \"model-{}.pt\".format(step)))\n",
    "        logger.info(\"Saving model parameters at step=%d\" % step)\n",
    "\n",
    "def do_train(num_training_steps, save_period, log_period, gradient_accumulation_steps=1, max_grad_norm=1.0):\n",
    "        global_step = 0\n",
    "        train_losses = []\n",
    "        dataloader2 = list(text_dataloader)\n",
    "        for epoch in range(epoches):\n",
    "                for step, batch in enumerate(dataloader):\n",
    "                        global_step += 1\n",
    "\n",
    "                        input_ids=batch[0].to(device)\n",
    "                        attention_mask=batch[1].to(device)\n",
    "                        token_type_ids=batch[2].to(device)\n",
    "                        if len(batch)==3:\n",
    "                                labels=None\n",
    "                        else:\n",
    "                                labels=batch[3].to(device)\n",
    "                        text_input_ids = dataloader2[global_step%len(dataloader2)][\"input_ids\"].to(device)\n",
    "                        text_attention_mask = dataloader2[global_step%len(dataloader2)][\"attention_mask\"].to(device)\n",
    "                        # loss = run_model_meta_icl(input_ids, attention_mask, token_type_ids=token_type_ids, labels=labels)\n",
    "                        loss = run_model_icl_distill(input_ids, attention_mask, token_type_ids=token_type_ids, labels=labels, step=global_step, text_input_ids=text_input_ids, text_attention_mask=text_attention_mask)\n",
    "                        loss = loss.mean()\n",
    "\n",
    "                        train_losses.append(loss.detach().cpu())\n",
    "\n",
    "                        loss.backward()\n",
    "\n",
    "                        if global_step % gradient_accumulation_steps == 0:\n",
    "                                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "                                optimizer.step()    # We have accumulated enought gradients\n",
    "                                if scheduler is not None:\n",
    "                                        scheduler.step()\n",
    "                                        model.zero_grad()\n",
    "\n",
    "                        if global_step % log_period == 0:\n",
    "                                logger.info(\"global step %d\\t train loss %.2f\" % (global_step, np.mean(train_losses)))\n",
    "                                train_losses = []\n",
    "\n",
    "                        if global_step % save_period == 0:\n",
    "                                save(global_step)\n",
    "\n",
    "                        if global_step==num_training_steps:\n",
    "                                break\n",
    "\n",
    "                if global_step==num_training_steps:\n",
    "                        break\n",
    "\n",
    "        logger.info(\"Finish training\")\n",
    "do_train(data, batch_size, num_training_steps, save_period, log_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
