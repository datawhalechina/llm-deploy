{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿™ä»½ä»£ç ä¿®æ”¹è‡ªä»“åº“ï¼š https://github.com/timinar/BabyLlama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è®­ç»ƒæ•™å¸ˆæ¨¡å‹GPT-2 å’Œ Llama\n",
    "\n",
    "> è®ºæ–‡ä¸­å†™åˆ°ï¼š\n",
    ">\n",
    "> \"The GPT-2 model has 24 layers, 16 attention heads, an embedding dimension of 1536, intermediate size of 6144, and maximum sequence length of 128, resulting in 705M parameters. It was trained for 6 epochs with a batch size of 256 and maximum learning rate3 of 2.5 Â· 10âˆ’4. The LLaMA model has 24 layers, 8 attention heads, a hidden size of 1024, intermediate size of 3072, and maximum sequence length of 256, resulting in 360M parameters. It was trained for 4 epochs with a batch size of 128 and maximum learning rate of 3 Â· 10âˆ’4.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/train_10M_clean/tokenized_GPT2TokenizerFast_16000.pt\n",
      "ğŸ”¥ æ•°æ®é›†æ€»å¤§å°: 16912909\n",
      "ğŸ”¥ ä¸ºäº†ç¼©çŸ­è®­ç»ƒæ—¶é—´ï¼Œè¿™é‡Œç¼©å‡ä¸º: 422822\n",
      "Loading data from data/dev_clean/tokenized_GPT2TokenizerFast_16000.pt\n",
      "ğŸ”¥ æ•°æ®é›†æ€»å¤§å°: 17428872\n",
      "ğŸ”¥ ä¸ºäº†ç¼©çŸ­è®­ç»ƒæ—¶é—´ï¼Œè¿™é‡Œç¼©å‡ä¸º: 435721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/PJLAB/gaoyufei/workdir/llm-deploy/docs/chapter2/code/BabyLlama/babylm_dataset.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data = torch.load(tokenized_file)\n"
     ]
    }
   ],
   "source": [
    "# å‡†å¤‡æ•°æ®\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import GPT2TokenizerFast\n",
    "from babylm_dataset import BabylmDataset\n",
    "from random import sample, seed\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "data_train_path = \"./data/train_10M_clean\"\n",
    "data_eval_path = \"./data/dev_clean\"\n",
    "tokenizer_path = \"./models/gpt-clean-16000.json\"\n",
    "\n",
    "SEQ_LENGTH = 128\n",
    "tokenizer = GPT2TokenizerFast(tokenizer_file= str(tokenizer_path))\n",
    "tokenizer.bos_token = \"<s>\"\n",
    "tokenizer.eos_token = \"</s>\"\n",
    "tokenizer.pad_token = \"<pad>\"\n",
    "tokenizer.model_max_length = SEQ_LENGTH\n",
    "\n",
    "# è¿›å…¥BsbylmDatasetç±»ï¼Œå¯ä»¥åœ¨åˆå§‹åŒ–å‡½æ•°ä¸­ä¿®æ”¹æ•°æ®é›†å¤§å°\n",
    "train_dataset = BabylmDataset(data_train_path, SEQ_LENGTH, tokenizer=tokenizer, random_chunk=True)\n",
    "full_eval_dataset = BabylmDataset(data_eval_path, SEQ_LENGTH, tokenizer=tokenizer, offset=0)\n",
    "\n",
    "seed(2024) # we fix the same subset for all models\n",
    "eval_indices = sample(range(len(full_eval_dataset)), 200)\n",
    "eval_dataset = Subset(full_eval_dataset, eval_indices)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/PJLAB/gaoyufei/anaconda3/envs/babyllama/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/PJLAB/gaoyufei/anaconda3/envs/babyllama/lib/python3.9/site-packages/transformers/training_args.py:1590: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ğŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31ea8b21a604828bf402e0d6aee4b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# è®­ç»ƒGPT2æ¨¡å‹\n",
    "from transformers import (\n",
    "    GPT2Config, GPT2LMHeadModel, \n",
    ")\n",
    "from transformers import Trainer, TrainingArguments\n",
    "model_config = GPT2Config(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        n_positions=2*tokenizer.model_max_length,\n",
    "        n_embd=1536,\n",
    "        n_layer=24,\n",
    "        n_head=16,\n",
    "        pad_token_id=tokenizer.convert_tokens_to_ids(\"<pad>\"),\n",
    "    )\n",
    "model = GPT2LMHeadModel(model_config)\n",
    "\n",
    "output_dir = \"./models/gpt2-teacher\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs=6,\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_total_limit=1,  # Set to zero to avoid saving\n",
    "    warmup_steps=300, \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=float(2.5e-4),\n",
    "    logging_steps=20,\n",
    "    fp16=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    torch_compile = False,\n",
    "    no_cuda = True,   # we use CPUï¼Œæ˜¾å¡è¶³å¤Ÿå¤§çš„è¯å¯ä»¥æ”¹ä¸ºFalse\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/PJLAB/gaoyufei/anaconda3/envs/babyllama/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/PJLAB/gaoyufei/anaconda3/envs/babyllama/lib/python3.9/site-packages/transformers/training_args.py:1590: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ğŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220bcd3b441f40609e1993ca00e9c9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 16.5414, 'grad_norm': 7.050642013549805, 'learning_rate': 1.9999999999999998e-05, 'epoch': 0.24}\n",
      "{'loss': 10.4487, 'grad_norm': 4.373862266540527, 'learning_rate': 3.9999999999999996e-05, 'epoch': 0.48}\n",
      "{'loss': 7.7475, 'grad_norm': 2.7600486278533936, 'learning_rate': 5.9999999999999995e-05, 'epoch': 0.72}\n",
      "{'loss': 6.8205, 'grad_norm': 2.7919507026672363, 'learning_rate': 7.999999999999999e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f30fb59390345fa83872f5663072f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.165042877197266, 'eval_runtime': 48.2064, 'eval_samples_per_second': 4.149, 'eval_steps_per_second': 0.519, 'epoch': 1.0}\n",
      "{'loss': 6.1967, 'grad_norm': 2.381103515625, 'learning_rate': 9.999999999999999e-05, 'epoch': 1.2}\n",
      "{'loss': 5.7663, 'grad_norm': 2.0445632934570312, 'learning_rate': 0.00011999999999999999, 'epoch': 1.45}\n",
      "{'loss': 5.6016, 'grad_norm': 1.9865704774856567, 'learning_rate': 0.00014, 'epoch': 1.69}\n",
      "{'loss': 5.4055, 'grad_norm': 2.2117745876312256, 'learning_rate': 0.00015999999999999999, 'epoch': 1.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd176b315e394edbb280861352f6e768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.207029342651367, 'eval_runtime': 43.6859, 'eval_samples_per_second': 4.578, 'eval_steps_per_second': 0.572, 'epoch': 2.0}\n",
      "{'loss': 5.113, 'grad_norm': 2.0187833309173584, 'learning_rate': 0.00017999999999999998, 'epoch': 2.17}\n",
      "{'loss': 4.9987, 'grad_norm': 1.7442786693572998, 'learning_rate': 0.00019999999999999998, 'epoch': 2.41}\n",
      "{'loss': 4.8864, 'grad_norm': 1.9827890396118164, 'learning_rate': 0.00021999999999999995, 'epoch': 2.65}\n",
      "{'loss': 4.7585, 'grad_norm': 1.594044804573059, 'learning_rate': 0.00023999999999999998, 'epoch': 2.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70818e0d10a4591ab1d61de22885413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.277788162231445, 'eval_runtime': 47.1181, 'eval_samples_per_second': 4.245, 'eval_steps_per_second': 0.531, 'epoch': 3.0}\n",
      "{'loss': 4.554, 'grad_norm': 1.9135853052139282, 'learning_rate': 0.00026, 'epoch': 3.13}\n",
      "{'loss': 4.4329, 'grad_norm': 1.7531003952026367, 'learning_rate': 0.00028, 'epoch': 3.37}\n",
      "{'loss': 4.4757, 'grad_norm': 1.717421293258667, 'learning_rate': 0.0003, 'epoch': 3.61}\n",
      "{'loss': 4.2759, 'grad_norm': 1.5224635601043701, 'learning_rate': 9.259748514523653e-05, 'epoch': 3.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77066ee69048466fb456e8cd49161a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.135180473327637, 'eval_runtime': 44.22, 'eval_samples_per_second': 4.523, 'eval_steps_per_second': 0.565, 'epoch': 4.0}\n",
      "{'train_runtime': 7478.7308, 'train_samples_per_second': 1.413, 'train_steps_per_second': 0.044, 'train_loss': 6.295483612152467, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./gpt2-teacher/tokenizer_config.json',\n",
       " './gpt2-teacher/special_tokens_map.json',\n",
       " './gpt2-teacher/vocab.json',\n",
       " './gpt2-teacher/merges.txt',\n",
       " './gpt2-teacher/added_tokens.json',\n",
       " './gpt2-teacher/tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è®­ç»ƒLlamaæ¨¡å‹\n",
    "from transformers import (\n",
    "    LlamaConfig, LlamaForCausalLM,  \n",
    ")\n",
    "from transformers import Trainer, TrainingArguments\n",
    "model_config = LlamaConfig(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        max_position_embeddings=2*tokenizer.model_max_length,\n",
    "        hidden_size=1024,\n",
    "        intermediate_size=3072,\n",
    "        num_hidden_layers=24,\n",
    "        num_attention_heads=8,\n",
    "        tie_word_embeddings=False,\n",
    "        pad_token_id=tokenizer.convert_tokens_to_ids(\"<pad>\"),\n",
    "    )\n",
    "model = LlamaForCausalLM(model_config)\n",
    "\n",
    "output_dir = \"./models/llama-teacher\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    num_train_epochs=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_total_limit=1,  # Set to zero to avoid saving\n",
    "    warmup_steps=300, \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=float(3e-4),\n",
    "    logging_steps=20,\n",
    "    fp16=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    torch_compile = False,\n",
    "    no_cuda=True   # we use CPUï¼Œæ˜¾å¡è¶³å¤Ÿå¤§çš„è¯å¯ä»¥æ”¹ä¸ºFalse\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
