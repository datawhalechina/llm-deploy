{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-PrunerÂÆûË∑µ\n",
    "> ÂèÇËÄÉÈìæÊé•Ôºöhttps://github.com/horseee/LLM-Pruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import random\n",
    "import torch\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from transformers import LlamaTokenizer\n",
    "from LLMPruner.models.hf_llama.modeling_llama import LlamaForCausalLM, LlamaRMSNorm\n",
    "\n",
    "import LLMPruner.torch_pruning as tp \n",
    "from LLMPruner.pruner import hf_llama_pruner as llama_pruner\n",
    "from LLMPruner.datasets.example_samples import get_examples\n",
    "from LLMPruner.templates.prompts import prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"Enoch/llama-7b-hf\"\n",
    "cache_dir = \"./llm_weights\"\n",
    "num_examples = 10\n",
    "iterative_steps = 1 #Ëø≠‰ª£Ê¨°Êï∞\n",
    "taylor = 'param_first'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_version = float('.'.join(torch.__version__.split('.')[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "# ËÆæÁΩÆÈöèÊú∫ÁßçÂ≠êÔºåÊñπ‰æøÂ§çÁé∞ÁªìÊûú\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc908d019934f0ebd934dece9e704ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    cache_dir=cache_dir,\n",
    "    low_cpu_mem_usage=True if torch_version >=1.9 else False\n",
    ")\n",
    "if device != \"cpu\":\n",
    "    model.half()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÊµãËØïÊ®°ÂûãÊïàÊûú\n",
    "def eval():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for prompt in prompts:\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\")['input_ids'].to(device)\n",
    "\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                max_length=128,\n",
    "                top_p=0.95,\n",
    "                temperature=1,\n",
    "            )\n",
    "            \n",
    "            result = tokenizer.decode(generation_output[0])\n",
    "            print(f\"result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: <s> I believe the meaning of life is to live everyday as though it is your last and never take anyone for granted. Everyone you meet is fighting their own battle and it's important to treat others with the compassion they need.\n",
      "A woman must be twice as good as a man to go half as far, and that‚Äôs being conservative. That‚Äôs why woman have always had to work harder. ~ Elizabeth Cady Stanton</s>\n",
      "result: <s>Simply put, the theory of relativity states that 1) the speed of light is a constant for all observers, and that 2) the laws of physics behave differently depending upon the moving speed of the observer. Einstein didn't invent either of those notions. The first had been observed in 1676 when Ole R√∏mer determined that the speed of light is constant from all moving objects on Earth. The second had been proposed in 1887 by Hendrik Lorentz and independently by Georges Lema√Ætre (the former first). R√∏mer, Lorentz and L\n",
      "result: <s>Building a website can be done in 10 simple steps:\n",
      "1. Set Your Goal\n",
      "Website development comes down to three basic building blocks: the purpose, the content and the technology. What is the purpose of the website? How does the site content support that purpose? And how does that purpose and content drive the technology we choose to build your website? These elements form the structure, or framework, of the website. When we get started, we have a simple discussion to determine your website goals, and then the project plan can get started.\n",
      "2. Determine Your Business Goals\n",
      "We will ask\n",
      "result: <s>Tweet: \"I hate it when my phone battery dies.\"\n",
      "Sentiment: Negative\n",
      "###\n",
      "Tweet: \"My day has been üëç\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"This is the link to the article\"\n",
      "Sentiment: Neutral\n",
      "###\n",
      "Tweet: \"This new music video was incredibile\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"The new iPhone 7 is soooooo good!\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"\n",
      "result: <s>Translate English to French:\n",
      "\n",
      "sea otter => loutre de mer\n",
      "\n",
      "peppermint => menthe poivr√©e\n",
      "\n",
      "plush girafe => girafe peluche\n",
      "\n",
      "cheese => fromage\n",
      "\n",
      "copper => cuivre\n",
      "\n",
      "### English/French translation\n",
      "\n",
      "See also: Category:Translations by Wiktionary\n",
      "\n",
      "Main articles: French-English translations, French/English dictionary, and English/French dictionary\n",
      "\n",
      "There have been many dictionaries and reference books that attempt to translate English-French, and French-\n"
     ]
    }
   ],
   "source": [
    "# Ë£ÅÂâ™ÂâçÊµãËØïÊ®°ÂûãÊïàÊûú\n",
    "eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad_(True)\n",
    "    \n",
    "before_pruning_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "forward_prompts = torch.tensor([\n",
    "    [    1,   306,  4658,   278,  6593,   310,  2834,   338],\n",
    "    [    1,  3439, 17632,  1925, 29892,   278,  6368,   310],\n",
    "]).to(device) # Only for building the dependency graph. Any input will be fine since the computation result are not taken into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‰ΩøÁî®Ê≥∞ÂãíÊñπÂºèËÆ°ÁÆóÈáçË¶ÅÊÄß\n",
    "imp = llama_pruner.TaylorImportance(group_reduction=\"sum\", taylor=taylor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂÆö‰πâÈúÄË¶ÅÁöÑÂèÇÊï∞\n",
    "kwargs = {\n",
    "        \"importance\": imp,\n",
    "        \"global_pruning\": True,\n",
    "        \"iterative_steps\": 1, #Ëø≠‰ª£Ê¨°Êï∞ \n",
    "        \"ch_sparsity\": 0.2, #Á®ÄÁñèÁéá\n",
    "        \"ignored_layers\":[],\n",
    "        \"channel_groups\": {\n",
    "        },\n",
    "        \"consecutive_groups\": {\n",
    "        layer.self_attn.q_proj: layer.self_attn.head_dim for layer in model.model.layers\n",
    "        },\n",
    "        \"customized_pruners\": {\n",
    "        LlamaRMSNorm: llama_pruner.hf_rmsnorm_pruner,\n",
    "        },\n",
    "        \"root_module_types\": None, \n",
    "        \"root_instances\": [model.model.layers[i].self_attn.q_proj for i in range(3, 31)] +\n",
    "                        [model.model.layers[i].mlp.gate_proj for i in range(3, 31)] #Ë£ÅÂâ™3-31Â±Ç\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning Attention Layer = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Pruning MLP Layer = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
      "Start Pruning\n",
      "Start Backwarding in iterative steps = 0...\n",
      "Loss = 2.48046875\n",
      "torch.Size([309120]) 56\n",
      "61824 0.2 309120\n",
      "After Iter 1/1, #parameters: 5978476544\n"
     ]
    }
   ],
   "source": [
    "print(\"Pruning Attention Layer = {}\".format(list(range(3, 31))))\n",
    "print(\"Pruning MLP Layer = {}\".format(list(range(3,31))))\n",
    "\n",
    "pruner = tp.pruner.MetaPruner(\n",
    "        model,\n",
    "        forward_prompts,\n",
    "        **kwargs\n",
    ")\n",
    "model.zero_grad()\n",
    "\n",
    "print(\"Start Pruning\")\n",
    "for i in range(iterative_steps):\n",
    "        example_prompts = get_examples('c4', tokenizer, num_examples, seq_len = 64).to(device)\n",
    "        print(\"Start Backwarding in iterative steps = {}...\".format(i))\n",
    "                \n",
    "        loss = model(example_prompts, labels=example_prompts).loss\n",
    "        print(\"Loss = {}\".format(loss))\n",
    "        loss.backward()\n",
    "\n",
    "        pruner.step()\n",
    "\n",
    "        after_pruning_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(\"After Iter {}/{}, #parameters: {}\".format(i+1, iterative_steps, after_pruning_parameters))\n",
    "\n",
    "        # modify inferece-related attributes\n",
    "        for layer in model.model.layers:\n",
    "                layer.self_attn.num_heads = layer.self_attn.q_proj.weight.data.shape[0] // layer.self_attn.head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the gradient in the model\n",
    "model.zero_grad()\n",
    "for name, module in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "                module.grad = None\n",
    "del pruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Param before: 6738415616, #Param after: 5978476544, Ratio = 88.7223%\n"
     ]
    }
   ],
   "source": [
    "print(\"#Param before: {}, #Param after: {}, Ratio = {:.4f}%\".format(before_pruning_parameters, after_pruning_parameters,  100.0*after_pruning_parameters/before_pruning_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Requirement: 26024.59228515625 MiB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print((\"Memory Requirement: {} MiB\\n\".format(torch.cuda.memory_allocated()/1024/1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‰øùÂ≠òË£ÅÂâ™ÂêéÁöÑÊ®°Âûã\n",
    "torch.save({\n",
    "            'model': model, \n",
    "            'tokenizer': tokenizer,\n",
    "        }, 'model_llm_pruner.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.half()\n",
    "model.to(device)\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0 \n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: <s>I believe the meaning of life is to find a job that is of real value, that involves real skills and real craftsmanship and real human interactions with real other people. The whole point of life is that in a sense the whole point of life is the whole point of life. Life does not have a point except life; life is life, the point is life; life itself, life is what life is.\n",
      "Life is the art of taking, the process of keeping, and the use of an abundance. Life is living. Life is living. Life is life. Life is life. Life is life. Life\n",
      "result: <s>Simply put, the theory of relativity states that 2 physical objects in motion relative to each other experience a difference in time and speed due to the relative speed of the 2 objects relative to each other. The difference is called relativistic time dilation.\n",
      "The following video uses a clock with a ticking mechanism as a demonstration.\n",
      "The video shows how 2 clocks are accelerating relative to each other and shows how their time rates differ. For example, clock B shows a delay relative to clock A and clock A shows a delay relative clock clock B.\n",
      "In the video, the clocks are acceler\n",
      "result: <s>Building a website can be done in 10 simple steps:\n",
      "1. Create Your Website Layout\n",
      "2. Define Your Functional Requirements\n",
      "4. Prepare a Style Guide\n",
      "5. Write the Content\n",
      "7. Test Your Website\n",
      "9. Make Post-Launch Changes\n",
      "10. Make Promotional Materials\n",
      "Website Layout: This is the starting point of your website development. It defines the structure, navigation, and appearance of your website. This determines how information is presented, navigated, and found, as well as a lot of other features that are common to every website.\n",
      "result: <s>Tweet: \"I hate it when my phone battery dies.\"\n",
      "Sentiment: Negative\n",
      "###\n",
      "Tweet: \"My day has been üëç\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"This is the link to the article\"\n",
      "Sentiment: Neutral\n",
      "###\n",
      "Tweet: \"This new music video was incredibile\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"This new music video is incredible\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"I watched this YouTube\n",
      "result: <s>Translate English to French:\n",
      "\n",
      "sea otter => loutre de mer\n",
      "\n",
      "peppermint => menthe poivr√©e\n",
      "\n",
      "plush girafe => girafe peluche\n",
      "\n",
      "cheese => fromage\n",
      "\n",
      "English to French translation\n",
      "\n",
      "Input a English/English word, e.g. sea otter, and then the French version, loutre de mer. This is the reverse of the translator.\n",
      "\n",
      "Hint: French and English dictionaries work well with word-based inputs, and words of Latin and Greek origin are easier to find in English dictionaries\n"
     ]
    }
   ],
   "source": [
    "# Ë£ÅÂâ™ÂêéÊµãËØïÊ®°ÂûãÊïàÊûú\n",
    "eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    input=None,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    max_new_tokens=128,\n",
    "    stream_output=False,\n",
    "    **kwargs,\n",
    "):\n",
    "    inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            max_length=max_new_tokens,\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    yield output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://38fc60b05fb5e39c23.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://38fc60b05fb5e39c23.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ‰ΩøÁî®ÂèØËßÜÂåñÁïåÈù¢ÊµãËØïÔºåÁΩëÂùÄ‰∏∫ip:7860\n",
    "gr.Interface(\n",
    "    fn=evaluate,\n",
    "    inputs=[\n",
    "        gr.components.Textbox(lines=2, label=\"Input\", placeholder=\"none\"),\n",
    "        gr.components.Slider(\n",
    "            minimum=0, maximum=1, value=1, label=\"Temperature\"\n",
    "        ),\n",
    "        gr.components.Slider(\n",
    "            minimum=0, maximum=1, value=0.95, label=\"Top p\"\n",
    "        ),\n",
    "        gr.components.Slider(\n",
    "            minimum=0, maximum=100, step=1, value=50, label=\"Top k\"\n",
    "        ),\n",
    "        gr.components.Slider(\n",
    "            minimum=1, maximum=2000, step=1, value=128, label=\"Max tokens\"\n",
    "        ),\n",
    "        gr.components.Checkbox(label=\"Stream output\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(\n",
    "            lines=5,\n",
    "            label=\"Output\",\n",
    "        )\n",
    "    ],\n",
    "    title=\"Evaluate Pruned Model\",\n",
    "    description=\"Pruned Model\",\n",
    ").queue().launch(server_name=\"0.0.0.0\", share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-pruner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
